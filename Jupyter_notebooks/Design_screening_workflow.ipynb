{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For quick and automated analysis, all designs' names are organized in a design_list.txt file. Simulation files for each design need to be organized in respectively named directories, subdivided into \"apo\" and \"holo\" directories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python libraries\n",
    "import numpy as np\n",
    "%pylab inline\n",
    "from glob import glob\n",
    "import re\n",
    "import string\n",
    "from scipy.stats import norm\n",
    "import math\n",
    "\n",
    "## pyEmma\n",
    "from __future__ import print_function\n",
    "import pyemma\n",
    "print(pyemma.version)\n",
    "coor=pyemma.coordinates\n",
    "import pyemma.plots as mplt\n",
    "\n",
    "# MDtraj\n",
    "import mdtraj as md\n",
    "\n",
    "# Matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import *\n",
    "from matplotlib import colors, ticker, cm\n",
    "import matplotlib.mlab as mlab\n",
    "\n",
    "# MDAnalysis\n",
    "import MDAnalysis\n",
    "from MDAnalysis import Universe\n",
    "from MDAnalysis.analysis.density import density_from_Universe\n",
    "from MDAnalysis.analysis.align import AlignTraj\n",
    "from MDAnalysis.analysis.waterdynamics import SurvivalProbability as SP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define path variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "designs_file = '/path/to/design_list.txt' # file containing all design names\n",
    "traj_path = '/path/to/root/directory/containing/trajectory/files/'  ## directory to where trajectory files are located\n",
    "outdir = '/path/to/results/root/directory/' # directory where analysis results will be saved to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Clusters (NOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract atom coordinates from trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apo    \n",
    "designs = open(designs_file, 'r')\n",
    "\n",
    "for line in designs:\n",
    "    name = line.partition(\"\\n\")[0]\n",
    "    print(name)\n",
    "    \n",
    "    ## Load trajectory\n",
    "    topfile = traj_path + str(name) + '/apo/' + str(name) +'_apo.prmtop'  ## prmtop file for apo simulations\n",
    "    traj_list = glob(traj_path + str(name) + '/apo/' + str(name) + '_apo_simulations*.nc')\n",
    "    traj_list.sort()# Creating a list with all trajectory files in ascending order\n",
    "    \n",
    "    feat = coor.featurizer(topfile)\n",
    "    indices = feat.select_Ca() ## get indices of Calphas\n",
    "\n",
    "    # Getting Ca-coordinates (coordinates are saved in x y z format)\n",
    "    feat.add_selection(indices)\n",
    "    inp = coor.source(traj_list, feat)\n",
    "    c = inp.get_output()\n",
    "    \n",
    "    # Saving data\n",
    "    save_object(c, outdir+'clustering/' + str(name) + '_apo_Calphas_coordinates_combinedRuns.dat')   ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Holo    \n",
    "designs = open(designs_file, 'r')\n",
    "\n",
    "for line in designs:\n",
    "    name = line.partition(\"\\n\")[0]\n",
    "    print(name)\n",
    "    \n",
    "    ## Load trajectory\n",
    "    topfile = traj_path + str(name) + '/holo/' + str(name) +'_holo.prmtop'  ## prmtop file for holo simulations\n",
    "    traj_list = glob(traj_path + str(name) + '/holo/' + str(name) + '_holo_simulations*.nc')\n",
    "    traj_list.sort()# Creating a list with all trajectory files in ascending order\n",
    "    \n",
    "    feat = coor.featurizer(topfile)\n",
    "    indices = feat.select_Ca() ## get indices of Calphas\n",
    "\n",
    "    # Getting Ca-coordinates (coordinates are saved in x y z format)\n",
    "    feat.add_selection(indices)\n",
    "    inp = coor.source(traj_list, feat)\n",
    "    c = inp.get_output()\n",
    "    \n",
    "    # Saving data\n",
    "    save_object(c, outdir+'clustering/' + str(name) + '_holo_Calphas_coordinates_combinedRuns.dat')   ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform RMSD clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## APO\n",
    "\n",
    "designs = open(designs_file, 'r')\n",
    "\n",
    "for line in designs:\n",
    "    name = line.partition(\"\\n\")[0]\n",
    "    print(name)\n",
    "\n",
    "    ## Load distance file\n",
    "    d = pickle.load(open(outdir+'clustering/' + str(name) + '_apo_Calphas_coordinates_combinedRuns.dat', 'rb'))\n",
    "    \n",
    "    #Clustering - change parameters if desired\n",
    "    cl = coor.cluster_regspace(data = d, dmin = 0.15, stride=1, metric = 'minRMSD')\n",
    "    print(cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## HOLO\n",
    "\n",
    "designs = open(designs_file, 'r')\n",
    "\n",
    "for line in designs:\n",
    "    name = line.partition(\"\\n\")[0]\n",
    "    print(name)\n",
    "\n",
    "    ## Load distance file\n",
    "    d = pickle.load(open(outdir+'clustering/' + str(name) + '_holo_Calphas_coordinates_combinedRuns.dat', 'rb'))\n",
    "    \n",
    "    #Clustering - change parameters if desired\n",
    "    cl = coor.cluster_regspace(data = d, dmin = 0.15, stride=1, metric = 'minRMSD')\n",
    "    print(cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protein RMSF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating design-specific RMSF files from the default rmsf_analysis.in file provided with this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Creating cpptraj input to do RMSF analysis\n",
    "\n",
    "designs = open(designs_file, 'r')\n",
    "\n",
    "for line in designs:\n",
    "    name = line.partition(\"\\n\")[0]\n",
    "    n = 109 # number of residues in the protein\n",
    "\n",
    "    for r in range (1,4):  ## Perform analysis for all 3 runs, one at a time\n",
    "        ## cpptraj wrap input     \n",
    "        with open(outdir+'RMSF/' + str(name) + '_RMSF.r0' + str(r) + '.in', 'w') as inp:\n",
    "            infile = open(outdir+'RMSF/rmsf_analysis.in', 'r')\n",
    "            i = 1\n",
    "            for line in infile:\n",
    "                if i == 1:\n",
    "                    inp.write('trajin '+traj_path+str(name)+ '/apo/' + str(name) +'_apo_simulation.r0'+ str(r) + '.nc\\n')\n",
    "                    i = i + 1\n",
    "                elif i == 4:\n",
    "                    inp.write('atomicfluct out rmsf_CAfitCA_'+str(name) + '_apo.r0' + str(r) +'.dat   @CA byres\\n')\n",
    "                    i = i + 1\n",
    "                elif i == 5:\n",
    "                    inp.write('atomicfluct out rmsf_RESfitCA_'+str(name) + '_apo.r0' + str(r) +'.dat :1-'+str(n) + ' byres\\n')\n",
    "                    i = i + 1\n",
    "                else:\n",
    "                    inp.write(line)\n",
    "                    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /path/to/design_list/file\n",
    "while IFS= read -r file\n",
    "        do\n",
    "            cd /path/to/results/root/directory/RMSF/\n",
    "            cpptraj /path/to/root/directory/containing/trajectory/files/\"$file\"/apo/\"$file\"_apo.prmtop \"$file\"_RMSF.r01.in > RMSF.r01.log\n",
    "            cpptraj /path/to/root/directory/containing/trajectory/files/\"$file\"/apo/\"$file\"_apo.prmtop \"$file\"_RMSF.r02.in > RMSF.r02.log\n",
    "            cpptraj /path/to/root/directory/containing/trajectory/files/\"$file\"/apo/\"$file\"_apo.prmtop \"$file\"_RMSF.r03.in > RMSF.r03.log\n",
    "        done < \"design_list.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Creating cpptraj input to do RMSF analysis\n",
    "\n",
    "designs = open(designs_file, 'r')\n",
    "\n",
    "for line in designs:\n",
    "    name = line.partition(\"\\n\")[0]\n",
    "    n = 109 # number of residues in the protein\n",
    "\n",
    "    for r in range (1,4):  ## Perform analysis for all 3 runs, one at a time\n",
    "        ## cpptraj wrap input     \n",
    "        with open(outdir+'RMSF/' + str(name) + '_RMSF.r0' + str(r) + '.in', 'w') as inp:\n",
    "            infile = open(outdir+'RMSF/rmsf_analysis.in', 'r')\n",
    "            i = 1\n",
    "            for line in infile:\n",
    "                if i == 1:\n",
    "                    inp.write('trajin '+traj_path+str(name)+ '/holo/' + str(name) +'_holo_simulation.r0'+ str(r) + '.nc\\n')\n",
    "                    i = i + 1\n",
    "                elif i == 4:\n",
    "                    inp.write('atomicfluct out rmsf_CAfitCA_'+str(name) + '_holo.r0' + str(r) +'.dat   @CA byres\\n')\n",
    "                    i = i + 1\n",
    "                elif i == 5:\n",
    "                    inp.write('atomicfluct out rmsf_RESfitCA_'+str(name) + '_holo.r0' + str(r) +'.dat :1-'+str(n) + ' byres\\n')\n",
    "                    i = i + 1\n",
    "                else:\n",
    "                    inp.write(line)\n",
    "                    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /path/to/design_list/file\n",
    "while IFS= read -r file\n",
    "        do\n",
    "            cd /path/to/results/root/directory/RMSF/\n",
    "            cpptraj /path/to/root/directory/containing/trajectory/files/\"$file\"/holo/\"$file\"_holo.prmtop \"$file\"_RMSF.r01.in > RMSF.r01.log\n",
    "            cpptraj /path/to/root/directory/containing/trajectory/files/\"$file\"/holo/\"$file\"_holo.prmtop \"$file\"_RMSF.r02.in > RMSF.r02.log\n",
    "            cpptraj /path/to/root/directory/containing/trajectory/files/\"$file\"/holo/\"$file\"_holo.prmtop \"$file\"_RMSF.r03.in > RMSF.r03.log\n",
    "        done < \"design_list.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SASA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving per-residue SASA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## APO\n",
    "\n",
    "designs = open(designs_file, 'r')\n",
    "\n",
    "for line in designs:\n",
    "    name = line.partition(\"\\n\")[0]\n",
    "    print(name)\n",
    "    \n",
    "    for r in range(1,4): ## Perform analysis for all 3 runs, one at a time\n",
    "    \n",
    "        ## Load trajectory\n",
    "        prmtop_path =  traj_path + str(name) + '/apo/' + str(name) +'_apo.prmtop'  ## prmtop file for apo simulations\n",
    "        traj_file = glob(traj_path + str(name) + '/apo/' + str(name) + '_apo_simulations.r0'+str(r)+'.nc')\n",
    "        new_traj = md.load(traj_file, top=prmtop_path, stride=100)\n",
    "        \n",
    "        # calculate SASA\n",
    "        topology = new_traj.topology\n",
    "        sasa = md.shrake_rupley(new_traj.atom_slice(topology.select('protein')),  probe_radius=0.14, n_sphere_points=960, mode='residue')\n",
    "        ## save as object\n",
    "        save_object(sasa, outdir+'SASA/' + str(name) + '_apo_sasa_stride100_r0'+str(r)+'.dat')   ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HOLO\n",
    "\n",
    "designs = open(designs_file, 'r')\n",
    "\n",
    "for line in designs:\n",
    "    name = line.partition(\"\\n\")[0]\n",
    "    print(name)\n",
    "    \n",
    "    for r in range(1,4): ## Perform analysis for all 3 runs, one at a time\n",
    "    \n",
    "        ## Load trajectory\n",
    "        prmtop_path =  traj_path + str(name) + '/holo/' + str(name) +'_holo.prmtop'  ## prmtop file for holo simulations\n",
    "        traj_file = glob(traj_path + str(name) + '/holo/' + str(name) + '_holo_simulations.r0'+str(r)+'.nc')\n",
    "        new_traj = md.load(traj_file, top=prmtop_path, stride=100)\n",
    "        \n",
    "        # calculate SASA\n",
    "        topology = new_traj.topology\n",
    "        sasa = md.shrake_rupley(new_traj.atom_slice(topology.select('protein')),  probe_radius=0.14, n_sphere_points=960, mode='residue')\n",
    "        ## save as object\n",
    "        save_object(sasa, outdir+'SASA/' + str(name) + '_holo_sasa_stride100_r0'+str(r)+'.dat')   ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating hydrophobic SASA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "designs = open(designs_file, 'r')\n",
    "\n",
    "apo = []\n",
    "stdev_apo = []\n",
    "holo = []\n",
    "stdev_holo = []\n",
    "\n",
    "for line in designs:\n",
    "    name = line.partition(\"\\n\")[0]\n",
    "    print(name)\n",
    "    \n",
    "    ## Load sample structure to get residue ids\n",
    "    prmtop_path =  traj_path + str(name) + '/apo/' + str(name) +'_apo.prmtop'\n",
    "    traj_file = glob(traj_path + str(name) + '/apo/' + str(name) + '_apo_simulations.r01.nc')\n",
    "    new_traj = md.load(traj_file, top=prmtop_path, stride=100)\n",
    "\n",
    "    topology = new_traj.topology\n",
    "    backbone = topology.select('name CA')\n",
    "    # list of hydrophobic residues\n",
    "    backbone_hydrophobic = topology.select('name CA and resname ALA or name CA and resname ILE or name CA and resname LEU or name CA and resname PHE or name CA and resname VAL or name CA and resname PRO or name CA and resname GLY or name CA and resname MET or name CA and resname TRP')\n",
    "\n",
    "    # Creat a list with the positions of hydrophobic residues in the SASA output\n",
    "    hydrophobic_res_column = []\n",
    "    for y in range (0, len(backbone_hydrophobic)):\n",
    "        for z in range (0, len(backbone)):\n",
    "            if backbone_hydrophobic[y] == backbone[z]:\n",
    "                hydrophobic_res_column.append(z)\n",
    "                \n",
    "    apo_runs = []\n",
    "    holo_runs = []\n",
    "    \n",
    "    for r in range(1,4):\n",
    "        sasa = pickle.load(open(outdir+'SASA/'+str(name) + '_apo_sasa_stride100_r0'+str(r)+'.dat', 'rb'))\n",
    "        hydrophobic_sasa = sasa[:50, hydrophobic_res_column] ## Get only columns corresponding to hydrophobic residues, for first 500 ns\n",
    "        apo_runs.append((hydrophobic_sasa.sum(axis=1))/len(hydrophobic_res_column))\n",
    "        \n",
    "        sasa = pickle.load(open(outdir+'SASA/'+ str(name) + '_holo_sasa_stride100_r0'+str(r)+'.dat', 'rb'))\n",
    "        hydrophobic_sasa = sasa[:50, hydrophobic_res_column] ## Get only columns corresponding to hydrophobic residues, for first 500 ns\n",
    "        holo_runs.append((hydrophobic_sasa.sum(axis=1))/len(hydrophobic_res_column))\n",
    "        \n",
    "    apo.append(np.average(np.concatenate(apo_runs,axis=0)))\n",
    "    stdev_apo.append(np.std(np.concatenate(apo_runs,axis=0)))\n",
    "    holo.append(np.average(np.concatenate(holo_runs,axis=0)))\n",
    "    stdev_holo.append(np.std(np.concatenate(holo_runs,axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ligand RMSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pdb file with the initial complex structure (with waters) used to initiate the simulations is needed as the reference structure to calculate RMSD, here called $designname_holo.pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we write design-specific RMSD files from the default ligand_RMSD.in file provided with this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Creating cpptraj input to do RMSD analysis\n",
    "\n",
    "designs = open(designs_file, 'r')\n",
    "\n",
    "for line in designs:\n",
    "    name = line.partition(\"\\n\")[0]\n",
    "    n = 109 # number of residues in the protein\n",
    "    \n",
    "    for r in range (1,4):\n",
    "        ## creating cpptraj RMSD input     \n",
    "        with open(outdir+'RMSD/' + str(name) + '_ligandRMSD.r0' + str(r) + '.in', 'w') as inp:\n",
    "            infile = open(outdir+'RMSD/ligand_RMSD.in', 'r')\n",
    "            i = 1\n",
    "            for line in infile:\n",
    "                if i == 1:\n",
    "                    inp.write('parm '+ outdir + 'RMSD/' + str(name) + '_holo.pdb\\n')\n",
    "                    i = i + 1\n",
    "                elif i == 3:\n",
    "                    inp.write('parm '+ traj_path + str(name) + '/holo/' + str(name) +'_holo.prmtop\\n')\n",
    "                    i = i + 1\n",
    "                elif i == 4:\n",
    "                    inp.write('trajin '+traj_path+str(name)+'/holo/'+str(name) +'_holo_simulation.r0' + str(r) + '.nc parmindex 1\\n')\n",
    "                    i = i + 1\n",
    "                elif i == 6:\n",
    "                    inp.write('reference '+ outdir + 'RMSD/'+ str(name) + '_holo.pdb parmindex 0\\n')\n",
    "                    i = i + 1\n",
    "                elif i == 8:\n",
    "                    inp.write('rms reference :1-' + str(n) +'@CA\\n')\n",
    "                    i = i + 1\n",
    "                elif i == 9:\n",
    "                    inp.write('rmsd :' + str(int(n)+1)+ ' reference nofit out ' + str(name) + '_ligandRMSD.r0' +str(r) +'.dat\\n')\n",
    "                    i = i + 1\n",
    "                else:\n",
    "                    inp.write(line)\n",
    "                    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /path/to/design_list/file\n",
    "while IFS= read -r file\n",
    "        do\n",
    "            cd /path/to/results/root/directory/RMSD/\n",
    "            cpptraj \"$file\"_holo.pdb \"$file\"_ligandRMSD.r01.in > RMSD.r01.log\n",
    "            cpptraj \"$file\"_holo.pdb \"$file\"_ligandRMSD.r02.in > RMSD.r02.log\n",
    "            cpptraj \"$file\"_holo.pdb \"$file\"_ligandRMSD.r03.in > RMSD.r03.log\n",
    "        done < \"design_list.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protein-ligand H bonds count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Getting ids of atoms that interact with the ligand for at least 1% of the time\n",
    "\n",
    "designs = open(designs_file, 'r')\n",
    "\n",
    "pairs = []\n",
    "\n",
    "for line in designs:\n",
    "    name = line.partition(\"\\n\")[0]\n",
    "    n = 109 # number of residues in the protein\n",
    "    \n",
    "    topfile =  traj_path + str(name) + '/holo/' + str(name) +'_holo.prmtop'  ## prmtop file for apo simulations\n",
    "    traj_list = glob(traj_path + str(name) + '/holo/' + str(name) + '_holo_simulations.r0*.nc')\n",
    "    traj_list.sort()# Creating a list with all trajectory files in ascending order\n",
    "    \n",
    "    new_traj = md.load(traj_list[0], top=topfile, stride=1)\n",
    "    \n",
    "    for r in range (1,len(traj_list)):\n",
    "        temp_traj= md.load(traj_list[r], top=topfile, stride=1)\n",
    "        new_traj = new_traj + temp_traj\n",
    "\n",
    "    hbonds = md.baker_hubbard(new_traj, periodic=False, freq=0.01)\n",
    "    label = lambda hbond : '%s -- %s' % (new_traj.topology.atom(hbond[0]), new_traj.topology.atom(hbond[2]))\n",
    "    design_pairs = []\n",
    "    for hbond in hbonds:\n",
    "        if str(new_traj.topology.atom(hbond[0]).residue) == 'MOL'+str(n) or str(new_traj.topology.atom(hbond[2]).residue) == 'MOL'+str(n):\n",
    "            print(label(hbond))\n",
    "            design_pairs.append(hbond)\n",
    "    pairs.append(design_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating angle and distances between the pairs\n",
    "\n",
    "designs = open(designs_file, 'r')\n",
    "\n",
    "d = 0 ## keeping count of which design we're in\n",
    "average_hbond_per_frame = []\n",
    "\n",
    "for line in designs:\n",
    "    name = line.partition(\"\\n\")[0]\n",
    "    print(name)\n",
    "    \n",
    "    hbonds =  np.array(pairs[d]) ## getting the hbonds ids of the design\n",
    "\n",
    "    topfile = traj_path + str(name) + '/holo/' + str(name) +'_holo.prmtop'  \n",
    "    \n",
    "    hbond_per_frame = []  ## to contain count for each frame of the combined runs\n",
    "    \n",
    "    for r in range(1,4):\n",
    "        traj_list = glob(traj_path + str(name) + '/holo/' + str(name) + '_holo_simulations.r0'+str(r)+'.nc')\n",
    "    \n",
    "        new_traj = md.load(traj_list[0], top=topfile, stride=1)\n",
    "                 \n",
    "        ## Calculate X-H --- Y distance\n",
    "        dist  = (md.compute_distances(new_traj, hbonds[:, [1,2]], periodic=False))*10  ## Calculating X-H --- Y distance (that is, distance between H and Y), in Angstroms\n",
    "    \n",
    "        ## Calculate X-H --- Y angle\n",
    "        a  = (md.compute_angles(new_traj, hbonds))*57.2958 ## Angle in degrees\n",
    "        \n",
    "        # save distances and angles as objects\n",
    "        save_object(dist, outdir +'Hbonds/'+ str(name) + '_all_protein_ligand_Hbonds_distances.r0'+str(r)+'.dat')\n",
    "        save_object(a, outdir +'Hbonds/' + str(name) + '_all_protein_ligand_Hbonds_angles.r0'+str(r)+'.dat')\n",
    "        \n",
    "    d = d + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting H bonds within angle and distance definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "designs = open(designs_list, 'r')\n",
    "\n",
    "average_hbond_per_frame = []\n",
    "stdev = []\n",
    "\n",
    "for line in designs:\n",
    "    name = line.partition(\"\\n\")[0]\n",
    "    print(name)\n",
    "    hbond_per_frame = []\n",
    "    \n",
    "    for r in range(1,4):\n",
    "        # open distances and angles as objects\n",
    "        a = pickle.load(open(outdir +'Hbonds/'+ str(name) +'_all_protein_ligand_Hbonds_angles.r0'+str(r)+'.dat', 'rb'))\n",
    "        dist = pickle.load(open(outdir +'Hbonds/'+ str(name) +'_all_protein_ligand_Hbonds_distances.r0'+str(r)+'.dat', 'rb'))\n",
    "        \n",
    "        ## Counting number of established H bonds per frame\n",
    "        for frame in range(0,len(dist)): ## For each frame\n",
    "            count = 0 # resetting cont\n",
    "            for pair in range(0,len(dist[0])): ## For each pair\n",
    "                if dist[frame][pair] < 3.2 and a[frame][pair] > 90: ## Using cutoff of 3.2 A and 90 degrees\n",
    "                    count = count + 1\n",
    "            hbond_per_frame.append(count)\n",
    "            \n",
    "    average_hbond_per_frame.append(np.average(hbond_per_frame))    \n",
    "    stdev.append(np.average(hbond_per_frame))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pocket volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving trajectories with a stride of 100, aligned to reference designs used to define inclusion spheres for POVME calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## APO\n",
    "\n",
    "designs = open(designs_list, 'r')\n",
    "\n",
    "for line in designs:\n",
    "    name = line.partition(\"\\n\")[0]\n",
    "    print(name)\n",
    "    \n",
    "    ref = indir + str(name) +'_aligned.pdb' ## Reference pdb to which trajectories are aligned to  \n",
    "    ref_traj = md.load(ref)\n",
    "    \n",
    "    for r in range (1,4):\n",
    "        traj_file = glob(traj_path + str(name) + '/apo/' + str(name) + '_apo_simulation.r0'+ str(r) +  '.nc')\n",
    "        prmtop_path = traj_path + str(name) + '/apo/' + str(name) +'_apo.prmtop'\n",
    "        \n",
    "        new_traj = md.load(traj_file, top=prmtop_path, stride=100)\n",
    "        only_protein = new_traj.atom_slice(atom_indices=new_traj.topology.select('protein'))\n",
    "        aligned_traj = only_protein.superpose(ref_traj, frame=0, atom_indices=only_protein.topology.select('protein and backbone'), ref_atom_indices=ref_traj.topology.select('protein and backbone') )\n",
    "        aligned_traj.save(outdir +'POVME/' + str(name) + '_apo_stride100_r0'+ str(r) +  '_aligned.pdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## APO\n",
    "\n",
    "designs = open(designs_list, 'r')\n",
    "\n",
    "for line in designs:\n",
    "    name = line.partition(\"\\n\")[0]\n",
    "    print(name)\n",
    "    \n",
    "    ref = indir + str(name) +'_aligned.pdb' ## Reference pdb to which trajectories are aligned to  \n",
    "    ref_traj = md.load(ref)\n",
    "    \n",
    "    for r in range (1,4):\n",
    "        traj_file = glob(traj_path + str(name) + '/holo/' + str(name) + '_holo_simulation.r0'+ str(r) +  '.nc')\n",
    "        prmtop_path = traj_path + str(name) + '/holo/' + str(name) +'_holo.prmtop'\n",
    "        \n",
    "        new_traj = md.load(traj_file, top=prmtop_path, stride=100)\n",
    "        only_protein = new_traj.atom_slice(atom_indices=new_traj.topology.select('protein'))\n",
    "        aligned_traj = only_protein.superpose(ref_traj, frame=0, atom_indices=only_protein.topology.select('protein and backbone'), ref_atom_indices=ref_traj.topology.select('protein and backbone') )\n",
    "        aligned_traj.save(outdir +'POVME/' + str(name) + '_holo_stride100_r0'+ str(r) +  '_aligned.pdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create input files for POVME, from default POVME_input.ini, in which the inclusion and seed spheres have been defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apo\n",
    "designs = open(designs_list, 'r')\n",
    "\n",
    "for line in designs:\n",
    "    name = line.partition(\"\\n\")[0]\n",
    "    print(name)\n",
    "    \n",
    "    for r in range(1,4):\n",
    "        with open(outdir + 'POVME/'+ str(name) + '_r0' + str(r) + '_apo_POVME.ini', 'w') as inp:\n",
    "    \n",
    "            infile = open(outdir + 'POVME/'+ 'POVME_input.ini', 'r')\n",
    "            i = 1\n",
    "            for line in infile:\n",
    "                if i == 14:\n",
    "                    inp.write('OutputFilenamePrefix  ' + str(name) + '_apo_stride100_r0' + str(r) +'_POVME2.0/\\n')\n",
    "                    i = i + 1\n",
    "                elif i == 15:\n",
    "                    inp.write('PDBFileName ' + str(name) +'_apo_stride100_r0' + str(r) + '_aligned.pdb\\n')\n",
    "                    i = i + 1\n",
    "                else:\n",
    "                    inp.write(line)\n",
    "                    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apo\n",
    "designs = open(designs_list, 'r')\n",
    "\n",
    "for line in designs:\n",
    "    name = line.partition(\"\\n\")[0]\n",
    "    print(name)\n",
    "    \n",
    "    for r in range(1,4):\n",
    "        with open(outdir + 'POVME/'+ str(name) + '_r0' + str(r) + '_holo_POVME.ini', 'w') as inp:\n",
    "    \n",
    "            infile = open(outdir + 'POVME/'+ 'POVME_input.ini', 'r')\n",
    "            i = 1\n",
    "            for line in infile:\n",
    "                if i == 14:\n",
    "                    inp.write('OutputFilenamePrefix  ' + str(name) + '_holo_stride100_r0' + str(r) +'_POVME2.0/\\n')\n",
    "                    i = i + 1\n",
    "                elif i == 15:\n",
    "                    inp.write('PDBFileName ' + str(name) +'_holo_stride100_r0' + str(r) + '_aligned.pdb\\n')\n",
    "                    i = i + 1\n",
    "                else:\n",
    "                    inp.write(line)\n",
    "                    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a list with all input files in order to run in bash\n",
    "\n",
    "designs = open(designs_file, 'r')\n",
    "\n",
    "with open(outdir + 'POVME/'+ 'POVME_input_files.txt', 'w') as inp:\n",
    "    for line in designs:\n",
    "        name = line.partition(\"\\n\")[0]\n",
    "        print(name)\n",
    "    \n",
    "        for r in range(1,4):\n",
    "            inp.write(str(name) + '_r0' + str(r) +'_apo_POVME.ini\\n')\n",
    "            inp.write(str(name) + '_r0' + str(r) +'_holo_POVME.ini\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run in terminal using the provided run.sh file with ./run.sh. Cavity volume per frame is saved in volumes.tabbed.txt in the respective output directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side chain dihedral angles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This hasn't been automated, so the desired residues for which to calculate dihedral angles need to be specified as exemplified below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select residues\n",
    "hbi10_residues = [10, 22, 50, 70, 88, 90, 98, 100]\n",
    "hbi10_residues_names = ['TRP', 'THR', 'PHE', 'TYR', 'PHE', 'PHE', 'THR', 'PHE']\n",
    "hbi11_residues = [10, 16, 22, 28, 70, 88, 90, 94]\n",
    "hbi11_residues_names = ['TRP', 'ASN', 'SER', 'PHE', 'TYR', 'PHE', 'PHE', 'THR']\n",
    "\n",
    "residues = [hbi10_residues,hbi11_residues]\n",
    "names = [hbi10_residues_names,hbi11_residues_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "designs = open(designs_list, 'r')\n",
    "\n",
    "d = 0 ## keeping count of which design we're in\n",
    "average_hbond_per_frame = []\n",
    "\n",
    "for line in designs:\n",
    "    name = line.partition(\"\\n\")[0]\n",
    "    print(name)\n",
    "    ids =  np.array(residues[d]) ## getting the residue ids of the design \n",
    "    \n",
    "    topfile = traj_path + str(name) + '/apo/' + str(name) +'_apo.prmtop'  \n",
    "    traj_list = glob(traj_path + str(name) + '/apo/' + str(name) + '_apo_simulations.r0*.nc')\n",
    "    traj_list.sort()# Creating a list with all trajectory files in ascending order\n",
    "    \n",
    "    new_traj = md.load(traj_list[0], top=topfile, stride=1)  \n",
    "    for r in range (1,len(traj_list)):\n",
    "        temp_traj= md.load(traj_list[r], top=topfile, stride=1)\n",
    "        new_traj = new_traj + temp_traj\n",
    "    \n",
    "    chi1_indices, chi1_angles = md.compute_chi1(new_traj)  \n",
    "    residues_chi1 = []\n",
    "    \n",
    "    for i in range(0, len(ids)): \n",
    "\n",
    "        alpha_c = new_traj.topology.select('resid ' + str(ids[i])+ ' and name CA')  \n",
    "        \n",
    "        dihedral_index = np.argwhere(chi1_indices == alpha_c)[0][0]\n",
    "        for f in range (len(chi1_angles[:, dihedral_index])):\n",
    "            chi1_angles[f,dihedral_index]=math.degrees(chi1_angles[f,dihedral_index])\n",
    "        residues_chi1.append(chi1_angles[:,dihedral_index])\n",
    "        time=range(len(chi1_angles[:,0]))\n",
    "        \n",
    "    ## Save object\n",
    "    save_object(residues_chi1, outdir +'dihedral_angle/' + str(name) + '_selected_atoms_dihedrals_apo.dat')\n",
    "\n",
    "    # Calculating % of frames with designed rotamer orientation\n",
    "    output = open(outdir +'dihedral_angle/'+str(name)+'_apo_%similarityDesignRotamer', 'w') ##\n",
    "    cutoff = 30.00  # angle cutoff range\n",
    "    ref_residue_id = 0\n",
    "\n",
    "    for dihedral in range (len(residues_chi1)): ##  For each dihedral\n",
    "        correct_rotamer = 0\n",
    "        for frame in range (len(residues_chi1[0])): ## For each frame\n",
    "            if ref_residues_chi1[ref_residue_id][0] <= -150: # to account for the -180 +180 angles\n",
    "                if ref_residues_chi1[ref_residue_id][0] + cutoff >= residues_chi1[dihedral][frame] or residues_chi1[dihedral][frame] >= 150: ## < -150 or > 150\n",
    "                    correct_rotamer = correct_rotamer + 1\n",
    "            elif ref_residues_chi1[ref_residue_id][0] >= 150: # to account for the -180 +180 angles\n",
    "                if ref_residues_chi1[ref_residue_id][0] - cutoff <= residues_chi1[dihedral][frame] or residues_chi1[dihedral][frame] <= -150: ## < -150 or > 150\n",
    "                    correct_rotamer = correct_rotamer + 1\n",
    "            else:\n",
    "                if ref_residues_chi1[ref_residue_id][0] - cutoff <= residues_chi1[dihedral][frame] <= ref_residues_chi1[ref_residue_id][0] + cutoff: ##\n",
    "                    correct_rotamer = correct_rotamer + 1\n",
    "        average_correct_rotamer = (correct_rotamer/float(len(residues_chi1[0]))) *100 ##\n",
    "        print(str(name) + ' apo '+ names[d][dihedral] + str(residues[d][dihedral]+1)+ ' Chi 1 % frames predicted rotamer = ' + str(average_correct_rotamer))\n",
    "        output.write(str(name) + ' holo '+ names[d][dihedral] + str(residues[d][dihedral]+1)+ ' Chi 1 % frames predicted rotamer = ' + str(average_correct_rotamer) + '\\n')\n",
    "        \n",
    "        ref_residue_id =ref_residue_id + 1\n",
    "    \n",
    "    output.close()\n",
    "        \n",
    "    d = d + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Water count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In here I use MDAnalysis' volume definition defined around a point in space as the center of a sphere. This is good because it's design independent, and I'm selecting the position of atom C4 in the ligand of the reference design as the center coordinate, since this is really at the center of the pocket. Simulations were aligned to the reference structure in order for this to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## APO\n",
    "\n",
    "designs = open(designs_file, 'r')\n",
    "\n",
    "c4 = re.compile(r'C4  HBI X')  ## C4 is the selected central atom to define the inclusion sphere for water calculation\n",
    "\n",
    "for line in designs:\n",
    "    name = line.partition(\"\\n\")[0]\n",
    "    print(name)\n",
    "    \n",
    "    ## Find coordinates of C4 atom\n",
    "    with open(outdir+ 'water_count/'+str(name) +'_holo.pdb', 'r') as ref:\n",
    "        for line in ref:\n",
    "            if c4.search(line):\n",
    "                columns = line.split()\n",
    "                x = float(columns[6])\n",
    "                y = float(columns[7])\n",
    "                z = float(columns[8])\n",
    "    print(x,y,z)\n",
    "    \n",
    "    runs = []\n",
    "\n",
    "    topology = traj_path + str(name) + '/apo/' + str(name) +'_apo.prmtop'\n",
    "    \n",
    "    ## calculate number of waters for each run\n",
    "    for r in range(1,4):\n",
    "        water_count = []\n",
    "        traj = glob(traj_path + str(name) + '/apo/' + str(name) + '_apo_simulation.r0'+str(r)+'.nc')\n",
    "        u = Universe(topology,traj)\n",
    "        group = u.select_atoms(\"resname WAT and name O and point \" +str(x)+ \" \" +str(y)+\" \"+str(z)+\" 7\",updating=True) ## Coordinates of ligand's atom C4\n",
    "        frame = 0\n",
    "        while frame < 4999: # number of frames in trajectory\n",
    "            water_count.append(len(group.positions)) ## positions gives the coordinates for each selected atom. \n",
    "                                                    ## Calculating the length of this list was the only way I found to give the number of water molecules\n",
    "            frame = frame + 1\n",
    "            u.trajectory.next() # going to next frame in the trajectory\n",
    "        runs.append(water_count)\n",
    "    \n",
    "    ## save object contaning the water count for each run, for each design\n",
    "    save_object(runs, outdir + 'water_count/'+ str(name) +'_apo_water_count_per_run.dat')   ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HOLO\n",
    "\n",
    "designs = open(designs_list, 'r')\n",
    "\n",
    "c4 = re.compile(r'C4  HBI X')  ## C4 is the selected central atom to define the inclusion sphere for water calculation\n",
    "\n",
    "for line in designs:\n",
    "    name = line.partition(\"\\n\")[0]\n",
    "    print(name)\n",
    "    \n",
    "    ## Find coordinates of C4 atom\n",
    "    with open(outdir+ 'water_count/'+str(name) +'_holo.pdb', 'r') as ref:\n",
    "        for line in ref:\n",
    "            if c4.search(line):\n",
    "                columns = line.split()\n",
    "                x = float(columns[6])\n",
    "                y = float(columns[7])\n",
    "                z = float(columns[8])\n",
    "    print(x,y,z)\n",
    "    \n",
    "    runs = []\n",
    "\n",
    "    topology = traj_path + str(name) + '/holo/' + str(name) +'_holo.prmtop'\n",
    "    \n",
    "    ## calculate number of waters for each run\n",
    "    for r in range(1,4):\n",
    "        water_count = []\n",
    "        traj = glob(traj_path + str(name) + '/holo/' + str(name) + '_holo_simulation.r0'+str(r)+'.nc')\n",
    "        u = Universe(topology,traj)\n",
    "        group = u.select_atoms(\"resname WAT and name O and point \" +str(x)+ \" \" +str(y)+\" \"+str(z)+\" 7\",updating=True) ## Coordinates of ligand's atom C4\n",
    "        frame = 0\n",
    "        while frame < 4999: # number of frames in trajectory\n",
    "            water_count.append(len(group.positions)) ## positions gives the coordinates for each selected atom. \n",
    "                                                    ## Calculating the length of this list was the only way I found to give the number of water molecules\n",
    "            frame = frame + 1\n",
    "            u.trajectory.next() # going to next frame in the trajectory\n",
    "        runs.append(water_count)\n",
    "    \n",
    "    ## save object contaning the water count for each run, for each design\n",
    "    save_object(runs, outdir + 'water_count/'+ str(name) +'_holo_water_count_per_run.dat')   ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Water survival probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating for apo simulations only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "designs = open(designs_list, 'r')\n",
    "\n",
    "c4 = re.compile(r'C4  HBI B')  ## C4 is the selected central atom to define the inclusion sphere for water calculation\n",
    "\n",
    "for line in designs:\n",
    "    name = line.partition(\"\\n\")[0]\n",
    "    print(name)\n",
    "    \n",
    "    ## Find coordinates of C4 atom\n",
    "    with open(outdir+ 'water_count/'+str(name) +'_holo.pdb', 'r') as ref:\n",
    "        for line in ref:\n",
    "            if c4.search(line):\n",
    "                columns = line.split()\n",
    "                x = float(columns[6])\n",
    "                y = float(columns[7])\n",
    "                z = float(columns[8])\n",
    "    print(x,y,z)\n",
    "    \n",
    "    topology = indir + 'water_survival/' + str(name) + '/apo/' + str(name) +'_apo.prmtop'\n",
    "    \n",
    "    ## calculate number of waters for each run\n",
    "    for r in range(1,4):\n",
    "        traj = glob(traj_path + str(name) + '/apo/' + str(name) + '_apo_simulations.r0'+str(r)+'.nc')\n",
    "        u = Universe(topology,traj)\n",
    "        selection = \"resname WAT and name O and point \" +str(x)+ \" \" +str(y)+\" \"+str(z)+\" 7\" ## Coordinates of ligand's atom C4\n",
    "        SP_analysis = SP(u, selection, 4000, 5000, 100)\n",
    "        SP_analysis.run()\n",
    "\n",
    "        ## save object \n",
    "        save_object(SP_analysis.timeseries, outdir + 'water_survival/' str(name) +'_apo_water_SP.r0' +str(r) +'.dat')   ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organize above calculated features in arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOC_apo = [...]\n",
    "NOC_holo = [...]\n",
    "SASA_apo = [...]\n",
    "SASA_holo = [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_apo = [...]\n",
    "vol_holo = [...]\n",
    "below_30_frame_count_apo = [...]\n",
    "vol_change_apo = [...]\n",
    "vol_change_holo = [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_count_apo =[...]\n",
    "water_count_holo =[...]\n",
    "ligand_rmsd = [...]\n",
    "protein_ligand_hbonds = [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating array with all features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "data.append(NOC_apo)\n",
    "data.append(NOC_holo)\n",
    "data.append(SASA_apo)\n",
    "data.append(SASA_holo)\n",
    "data.append(vol_apo)\n",
    "data.append(vol_holo)\n",
    "data.append(below_30_frame_count_apo)\n",
    "data.append(vol_change_apo)\n",
    "data.append(vol_change_holo)\n",
    "data.append(water_count_apo)\n",
    "data.append(water_count_holo)\n",
    "data.append(ligand_rmsd)\n",
    "data.append(protein_ligand_hbonds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['Apo NOC','Holo NOC','Apo SASA','Holo SASA','Apo cavity volume','Holo cavity volume','Apo volume frame count <cutoff','Apo volume change','Holo volume change','Apo cavity water count','Holo cavity water count','Ligand RMSD','Protein-ligand H bonds count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.transpose(np.asarray(data))\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## This scales each feature in the array (that is, each column in the features array) to range from 0 to 1 according\n",
    "## to its minimum and maximun values\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "print(scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA with three components\n",
    "pca3 = PCA(n_components=3)\n",
    "reduced_cartesian_scaled = pca3.fit_transform(scaled_features)\n",
    "print(reduced_cartesian_scaled.shape)\n",
    "print(pca3.explained_variance_ratio_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3D scatter\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for i in range(0,len(design_names)):\n",
    "    if design_names[i] in good_binders:\n",
    "        ax.scatter(reduced_cartesian_scaled[i, 0],  reduced_cartesian_scaled[i,1], reduced_cartesian_scaled[i,2], color = 'darkturquoise', s=50)\n",
    "    elif design_names[i] in unstable:\n",
    "        ax.scatter(reduced_cartesian_scaled[i, 0],  reduced_cartesian_scaled[i,1], reduced_cartesian_scaled[i,2], color = 'black', s=50)\n",
    "    else:\n",
    "        ax.scatter(reduced_cartesian_scaled[i, 0],  reduced_cartesian_scaled[i,1], reduced_cartesian_scaled[i,2], color = 'orangered', s=50)\n",
    "\n",
    "axes=plt.gca()        \n",
    "\n",
    "ax.set_xlabel('Principal Component 1',fontsize=12)\n",
    "ax.set_ylabel('Principal Component 2',fontsize=12)\n",
    "ax.set_zlabel('Principal Component 3',fontsize=12)\n",
    "\n",
    "#fig.savefig('/scratch/epecoradebarros/Baker_project/Analysis_paper/298K_biggerBox/model/Bbarrel_3PCA_plot.pdf', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(pca3.components_,cmap='viridis')\n",
    "plt.yticks([0,1,2],['PC 1','PC 2','PC 3'],fontsize=12)\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('Feature contribution',fontsize=12)\n",
    "plt.xticks(range(len(feature_names)),feature_names,rotation=65,ha='left',fontsize=16)\n",
    "#plt.savefig('/gpfs/amarolab/epecoradebarros/texas_scratch/Baker_project/Analysis_paper/298K_biggerBox/model/Bbarrel_3PC_feature_contriburion_largerFont.pdf', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering -  3 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import KMeans\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create kmeans object\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "# fit kmeans object to data\n",
    "kmeans.fit(reduced_cartesian_scaled)\n",
    "# print location of clusters learned by kmeans object\n",
    "print(kmeans.cluster_centers_)\n",
    "# save new clusters for chart\n",
    "y_km = kmeans.fit_predict(reduced_cartesian_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Seeing results\n",
    "plt.scatter(reduced_cartesian_scaled[y_km ==0,0], reduced_cartesian_scaled[y_km == 0,1], s=100, c='red')\n",
    "plt.scatter(reduced_cartesian_scaled[y_km ==1,0], reduced_cartesian_scaled[y_km == 1,1], s=100, c='black')\n",
    "plt.scatter(reduced_cartesian_scaled[y_km ==2,0], reduced_cartesian_scaled[y_km == 2,1], s=100, c='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score, matthews_corrcoef, precision_score, recall_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organize features and labels for data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features\n",
    "y = [0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,0]\n",
    "## labels: 0 for non-binder, 1 for binder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing k-fold cross validation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc = []\n",
    "roc = []\n",
    "f1s = []\n",
    "acc = []\n",
    "prec = []\n",
    "rec = []\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle = True)  ## n_splits = 5 for 5-fold cross validation\n",
    "kf.get_n_splits(X)\n",
    "print(kf)  \n",
    "for train_index, test_index in kf.split(X):\n",
    "#    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_valid = X[train_index], X[test_index]\n",
    "    y_train, y_valid = np.array(y)[train_index], np.array(y)[test_index]\n",
    "    print(y_train)\n",
    "    \n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_valid = scaler.transform(X_valid)\n",
    "    \n",
    "    # Fitting a logistic regression\n",
    "    lrc = LogisticRegression().fit(X_train,y_train)\n",
    "    \n",
    "    # Validation\n",
    "    probs = lrc.predict_proba(X_valid)[:,1]\n",
    "    preds = lrc.predict(X_valid)\n",
    "    mcc.append(matthews_corrcoef(y_valid, preds))\n",
    "    roc.append(roc_auc_score(y_valid, probs))\n",
    "    f1s.append(f1_score(y_valid, preds))\n",
    "    acc.append(lrc.score(X_valid,y_valid))\n",
    "    prec.append(precision_score(y_valid, preds))\n",
    "    rec.append(recall_score(y_valid, preds))\n",
    "\n",
    "print('mcc = '+str(np.average(mcc))+' +- '+str(np.std(mcc)))\n",
    "print('f1s = '+str(np.average(f1s))+' +- '+str(np.std(f1s)))\n",
    "print('accuracy = '+str(np.average(acc))+' +- '+str(np.std(acc)))\n",
    "print('precision = '+str(np.average(prec))+' +- '+str(np.std(prec)))\n",
    "print('recall = '+str(np.average(rec))+' +- '+str(np.std(rec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- k-nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc = []\n",
    "roc = []\n",
    "f1s = []\n",
    "acc = []\n",
    "prec = []\n",
    "rec = []\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle = True)  ## n_splits = 5 for 5-fold cross validation\n",
    "kf.get_n_splits(X)\n",
    "print(kf)  \n",
    "for train_index, test_index in kf.split(X):   \n",
    "#    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_valid = X[train_index], X[test_index]\n",
    "    y_train, y_valid = np.array(y)[train_index], np.array(y)[test_index]\n",
    "    print(y_train)\n",
    "    \n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_valid = scaler.transform(X_valid)\n",
    "    \n",
    "    # Fitting a logistic regression\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X_train,y_train)\n",
    "    \n",
    "    # Validation\n",
    "    probs = knn.predict_proba(X_valid)[:,1]\n",
    "    preds = knn.predict(X_valid)\n",
    "    mcc.append(matthews_corrcoef(y_valid, preds))\n",
    "    roc.append(roc_auc_score(y_valid, probs))\n",
    "    f1s.append(f1_score(y_valid, preds))\n",
    "    acc.append(knn.score(X_valid,y_valid))\n",
    "    prec.append(precision_score(y_valid, preds))\n",
    "    rec.append(recall_score(y_valid, preds))\n",
    "\n",
    "print('mcc = '+str(np.average(mcc))+' +- '+str(np.std(mcc)))\n",
    "print('f1s = '+str(np.average(f1s))+' +- '+str(np.std(f1s)))\n",
    "print('accuracy = '+str(np.average(acc))+' +- '+str(np.std(acc)))\n",
    "print('precision = '+str(np.average(prec))+' +- '+str(np.std(prec)))\n",
    "print('recall = '+str(np.average(rec))+' +- '+str(np.std(rec)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
